{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lex1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOqsjG1FWB7Mq06HR4VrSZI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"gQPHE_DC-IK1","colab_type":"code","colab":{}},"source":["# Importing libraries\n","import pandas as pd\n","import tensorflow as tf\n","import keras\n","# Reading train and test data\n","train_data = pd.read_csv('fashionmnisttrain.csv')\n","# Class names\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","# Creating validation data from test data\n","val_data = train_data.iloc[:5000,:]\n","test_data = train_data.iloc[5000:,:]\n","# Fetching the labels\n","train_labels = train_data.label\n","val_labels = val_data.label\n","test_labels = test_data.label\n","# Reshaping training data\n","train_images = train_data.iloc[:,1:].values.reshape(60000, 28, 28)\n","# Reshaping validation data\n","val_images = val_data.iloc[:,1:].values.reshape(5000, 28, 28)\n","# Scaling data in the range of 0-1\n","train_images = train_images/255.0\n","val_images = val_images/255.0\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQnNICRIDsmv","colab_type":"code","outputId":"791a795b-9c54-4c2f-c433-70f8e1a3448e","executionInfo":{"status":"ok","timestamp":1587180876491,"user_tz":-330,"elapsed":16275,"user":{"displayName":"Pradeep Chauhan","photoUrl":"","userId":"05150516264522047115"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["# Defining multi-layer perceptron model with 1 hidden layer having 1 neuron\n","model = keras.Sequential([\n","    keras.layers.Flatten(input_shape=(28, 28)), # Perform conversion of higher dimensional data (here, 2-D) to 1-D data.\n","    keras.layers.Dense(1, activation=tf.keras.activations.linear), # Hidden layer with 1 neuron and linear activation function\n","    keras.layers.Dense(10, activation=tf.keras.activations.linear) # Output layer with linear activation function \n","])                                                   \n","# Defining parameters like optimizer, loss function and evaluating metric\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer=keras.optimizers.Adam(), \n","              metrics=['accuracy'])\n","model1 = model.fit(train_images, train_labels, epochs=5, validation_data=(val_images, val_labels))\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 5000 samples\n","Epoch 1/5\n","60000/60000 [==============================] - 3s 53us/step - loss: 4.4112 - accuracy: 0.1428 - val_loss: 4.7127 - val_accuracy: 0.1182\n","Epoch 2/5\n","60000/60000 [==============================] - 3s 48us/step - loss: 2.8905 - accuracy: 0.1395 - val_loss: 2.3005 - val_accuracy: 0.1032\n","Epoch 3/5\n","60000/60000 [==============================] - 3s 48us/step - loss: 2.3018 - accuracy: 0.0962 - val_loss: 2.3026 - val_accuracy: 0.0998\n","Epoch 4/5\n","60000/60000 [==============================] - 3s 47us/step - loss: 2.3026 - accuracy: 0.0958 - val_loss: 2.3026 - val_accuracy: 0.0998\n","Epoch 5/5\n","60000/60000 [==============================] - 3s 47us/step - loss: 2.3026 - accuracy: 0.0958 - val_loss: 2.3026 - val_accuracy: 0.0998\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vA_KWcGnWvPc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"8aba3085-efd2-4696-b4bf-ef7de5d88c47","executionInfo":{"status":"ok","timestamp":1587180956993,"user_tz":-330,"elapsed":17408,"user":{"displayName":"Pradeep Chauhan","photoUrl":"","userId":"05150516264522047115"}}},"source":["# Defining multi-layer perceptron model with 1 hidden layer having 10 neurons \n","model = keras.Sequential([\n","    keras.layers.Flatten(input_shape=(28, 28)), # Perform conversion of higher dimensional data (here, 2-D) to 1-D data.\n","    keras.layers.Dense(10, activation=tf.keras.activations.linear), # Hidden layer with 10 neurons and linear activation function\n","    keras.layers.Dense(10, activation=tf.keras.activations.linear) # Output layer with linear activation function \n","])\n","# Defining parameters like optimizer, loss function and evaluating metric\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer=keras.optimizers.Adam(), \n","              metrics=['accuracy'])\n","model2 = model.fit(train_images, train_labels, epochs=5, validation_data=(val_images, val_labels))\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 5000 samples\n","Epoch 1/5\n","60000/60000 [==============================] - 3s 53us/step - loss: 3.3727 - accuracy: 0.1821 - val_loss: 1.7283 - val_accuracy: 0.1190\n","Epoch 2/5\n","60000/60000 [==============================] - 3s 55us/step - loss: 1.7558 - accuracy: 0.1476 - val_loss: 1.8101 - val_accuracy: 0.2430\n","Epoch 3/5\n","60000/60000 [==============================] - 3s 53us/step - loss: 1.8061 - accuracy: 0.1689 - val_loss: 1.7807 - val_accuracy: 0.1952\n","Epoch 4/5\n","60000/60000 [==============================] - 3s 54us/step - loss: 1.6211 - accuracy: 0.2344 - val_loss: 1.5142 - val_accuracy: 0.2878\n","Epoch 5/5\n","60000/60000 [==============================] - 3s 53us/step - loss: 1.5783 - accuracy: 0.2969 - val_loss: 1.5349 - val_accuracy: 0.3138\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GSbvm9KSWyiy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"f1895dd6-87f0-4468-cca0-37dd98996fdb","executionInfo":{"status":"ok","timestamp":1587181001196,"user_tz":-330,"elapsed":16285,"user":{"displayName":"Pradeep Chauhan","photoUrl":"","userId":"05150516264522047115"}}},"source":["# Defining multi-layer perceptron model with 1 hidden layer having 10 neurons with non-linearity\n","model = keras.Sequential([\n","    keras.layers.Flatten(input_shape=(28, 28)), # Perform conversion of higher dimensional data (here, 2-D) to 1-D data.\n","    keras.layers.Dense(10, activation=tf.nn.relu), # Hidden layer with 10 neurons and ReLU activation function\n","    keras.layers.Dense(10, activation=tf.nn.softmax) # Output layer with softmax activation function \n","])\n","# Defining parameters like optimizer, loss function and evaluating metric\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer=keras.optimizers.Adam(), \n","              metrics=['accuracy'])\n","model3 = model.fit(train_images, train_labels, epochs=5, validation_data=(val_images, val_labels))\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 5000 samples\n","Epoch 1/5\n","60000/60000 [==============================] - 3s 51us/step - loss: 0.6669 - accuracy: 0.7677 - val_loss: 0.4807 - val_accuracy: 0.8370\n","Epoch 2/5\n","60000/60000 [==============================] - 3s 50us/step - loss: 0.4687 - accuracy: 0.8358 - val_loss: 0.4376 - val_accuracy: 0.8492\n","Epoch 3/5\n","60000/60000 [==============================] - 3s 50us/step - loss: 0.4398 - accuracy: 0.8462 - val_loss: 0.4225 - val_accuracy: 0.8572\n","Epoch 4/5\n","60000/60000 [==============================] - 3s 50us/step - loss: 0.4261 - accuracy: 0.8510 - val_loss: 0.4125 - val_accuracy: 0.8590\n","Epoch 5/5\n","60000/60000 [==============================] - 3s 50us/step - loss: 0.4175 - accuracy: 0.8531 - val_loss: 0.4026 - val_accuracy: 0.8644\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xgdSSYW5W9lI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"e745180e-ea7d-4ecb-c80c-c90f4795932b","executionInfo":{"status":"ok","timestamp":1587181045860,"user_tz":-330,"elapsed":17811,"user":{"displayName":"Pradeep Chauhan","photoUrl":"","userId":"05150516264522047115"}}},"source":["# Defining multi-layer perceptron model with 3 hidden layer having 10 neurons each and with non-linearity\n","model = keras.Sequential([\n","    keras.layers.Flatten(input_shape=(28, 28)), # Perform conversion of higher dimensional data (here, 2-D) to 1-D data.\n","    keras.layers.Dense(10, activation=tf.nn.relu), # Hidden layer with 10 neurons and ReLU activation function\n","    keras.layers.Dense(10, activation=tf.nn.relu), # Hidden layer with 10 neurons and ReLU activation function\n","    keras.layers.Dense(10, activation=tf.nn.relu), # Hidden layer with 10 neurons and ReLU activation function\n","    keras.layers.Dense(10, activation=tf.nn.softmax) # Output layer with softmax activation function \n","])\n","# Defining parameters like optimizer, loss function and evaluating metric\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer=keras.optimizers.Adam(),\n","              metrics=['accuracy'])\n","model4 = model.fit(train_images, train_labels, epochs=5, validation_data=(val_images, val_labels))\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 5000 samples\n","Epoch 1/5\n","60000/60000 [==============================] - 3s 56us/step - loss: 0.7491 - accuracy: 0.7217 - val_loss: 0.5112 - val_accuracy: 0.8264\n","Epoch 2/5\n","60000/60000 [==============================] - 3s 55us/step - loss: 0.5078 - accuracy: 0.8222 - val_loss: 0.4556 - val_accuracy: 0.8450\n","Epoch 3/5\n","60000/60000 [==============================] - 3s 54us/step - loss: 0.4743 - accuracy: 0.8342 - val_loss: 0.4408 - val_accuracy: 0.8444\n","Epoch 4/5\n","60000/60000 [==============================] - 3s 54us/step - loss: 0.4531 - accuracy: 0.8415 - val_loss: 0.4243 - val_accuracy: 0.8532\n","Epoch 5/5\n","60000/60000 [==============================] - 3s 54us/step - loss: 0.4395 - accuracy: 0.8463 - val_loss: 0.4165 - val_accuracy: 0.8548\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XvvSVrbnXII8","colab_type":"code","colab":{}},"source":["# Necessary libraries to be used \n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","%matplotlib notebook\n","# X: (Feature 1, Feature 2)\n","X = np.array([[5, 40], \n","              [8, 82], \n","              [6, 52]], dtype=float)\n","# y: Target\n","y = np.array([[15], [24], [18]], dtype=float)\n","# Scaling units\n","X = X/np.max(X, axis=0) # maximum of X array\n","y = y/max(y) # maximum of y array\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HrxodSUWmHkk","colab_type":"code","colab":{}},"source":["class Neural_Network(object):\n","    def __init__(self):\n","        # Parameters\n","        self.inputSize = 2 # Two nodes\n","        self.outputSize = 1 # Single node\n","        self.hiddenSize = 3 # Three nodes\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gd82moWVmJMI","colab_type":"code","colab":{}},"source":["fuel = 5/8 \n","dist = 40/82\n","budget = 15/24\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cChq37K4mUut","colab_type":"code","colab":{}},"source":["fuelw1 = 0.3\n","fuelw2 = 0.2 \n","fuelw3 = 0.6\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RctOLNizme9N","colab_type":"code","colab":{}},"source":["distw1 = 0.22 \n","distw2 = 0.56 \n","distw3 = 0.7\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tPNHZc_mqBr","colab_type":"code","colab":{}},"source":["HN1 = (0.625 * 0.3) + (0.487 * 0.22)\n","HN2 = (0.625 * 0.2) + (0.487 * 0.56)\n","HN3 = (0.625 * 0.6) + (0.487 * 0.7)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1iheknR2m3Qk","colab_type":"code","colab":{}},"source":["def sigmoid(s):\n","      return 1/(1+np.exp(-s))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XuCy2XJGncgL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c26d92a7-270d-4bc9-872f-c203858ed08e","executionInfo":{"status":"ok","timestamp":1587185381140,"user_tz":-330,"elapsed":1669,"user":{"displayName":"Pradeep Chauhan","photoUrl":"","userId":"05150516264522047115"}}},"source":["sigmoid(HN1)\n","sigmoid(HN2)\n","sigmoid(HN3)\n"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6717035309940436"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"RKRwbDrDnlbw","colab_type":"code","colab":{}},"source":["Hw1 = 0.21 \n","Hw2 = 0.45 \n","Hw3 = 0.85\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLBGiPPMn4QL","colab_type":"code","colab":{}},"source":["o = HN1 * Hw1 + HN2 * Hw2 + HN3 * Hw3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"keGpg7UUn-6_","colab_type":"code","colab":{}},"source":["def sigmoidPrime(s):\n","    #derivative of sigmoid\n","    return s * (1 - s)\n","def backward(X, y, o):\n","    # backward propagation through the network\n","    o_error = y - o # error in output\n","    o_delta = o_error*sigmoidPrime(o) # applying derivative of sigmoid to error\n","    z2_error = o_delta.dot(W2.T) # z2 error: how much our hidden layer weights contributed to output error\n","    z2_delta = z2_error*sigmoidPrime(z2) # applying derivative of sigmoid to z2 error\n","    W1 += X.T.dot(z2_delta) # adjusting first set (input --> hidden) weights\n","    W2 += z2.T.dot(o_delta) # adjusting second set (hidden --> output) weights\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKYAUZR2rSRr","colab_type":"code","colab":{}},"source":["# Necessary libraries to be used \n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","%matplotlib notebook\n","# X: (Feature 1, Feature 2)\n","X = np.array([[5, 54, 24], \n","              [8, 68, 6], \n","              [9, 91, 100]], dtype=float)\n","# y: Target\n","y = np.array([[12], [16], [42]], dtype=float)\n","val_x = X[:2, :]\n","val_y = y[:2, :]\n","# Scaling units\n","X = X.reshape(3, 1, 3) # left to right 3 is 3 matrices , 1 row in each metric, 3 values in each row\n","val_x = val_x.reshape(2, 1, 3)\n","\n","X = X/np.mean(X, axis = 0 )\n","val_x = val_x/np.mean(val_x, axis = 0)\n","y = y/np.mean(y)\n","val_y = val_y/np.mean(val_y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2A9d6jhsq9u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4d7b888c-a5ca-4837-ec41-91f4ae5c4727","executionInfo":{"status":"ok","timestamp":1587195857045,"user_tz":-330,"elapsed":2041,"user":{"displayName":"Pradeep Chauhan","photoUrl":"","userId":"05150516264522047115"}}},"source":["# Defining multi-layer perceptron model with 3 hidden layer having 10 neurons each and with non-linearity\n","model = keras.Sequential([\n","    keras.layers.Flatten(input_shape=(1,3)), # Perform conversion of higher dimensional data (here, 2-D) to 1-D data.\n","    keras.layers.Dense(3, activation=tf.nn.relu), # Hidden layer with 10 neurons and ReLU activation function\n","    keras.layers.Dense(3, activation=tf.nn.relu), # Hidden layer with 10 neurons and ReLU activation function\n","    keras.layers.Dense(3, activation=tf.nn.sigmoid) # Output layer with softmax activation function \n","])\n","# Defining parameters like optimizer, loss function and evaluating metric\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer=keras.optimizers.SGD(),\n","              metrics=['accuracy'])\n","model4 = model.fit(X, y, epochs=50, validation_data=(val_x, val_y))\n"],"execution_count":117,"outputs":[{"output_type":"stream","text":["Train on 3 samples, validate on 2 samples\n","Epoch 1/50\n","3/3 [==============================] - 0s 23ms/step - loss: 1.1727 - accuracy: 0.0000e+00 - val_loss: 1.3297 - val_accuracy: 0.0000e+00\n","Epoch 2/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1672 - accuracy: 0.0000e+00 - val_loss: 1.3249 - val_accuracy: 0.0000e+00\n","Epoch 3/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1618 - accuracy: 0.0000e+00 - val_loss: 1.3201 - val_accuracy: 0.0000e+00\n","Epoch 4/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1564 - accuracy: 0.0000e+00 - val_loss: 1.3155 - val_accuracy: 0.0000e+00\n","Epoch 5/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1512 - accuracy: 0.0000e+00 - val_loss: 1.3110 - val_accuracy: 0.0000e+00\n","Epoch 6/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1461 - accuracy: 0.0000e+00 - val_loss: 1.3066 - val_accuracy: 0.0000e+00\n","Epoch 7/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1411 - accuracy: 0.0000e+00 - val_loss: 1.3023 - val_accuracy: 0.0000e+00\n","Epoch 8/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1361 - accuracy: 0.0000e+00 - val_loss: 1.2981 - val_accuracy: 0.0000e+00\n","Epoch 9/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1313 - accuracy: 0.0000e+00 - val_loss: 1.2939 - val_accuracy: 0.0000e+00\n","Epoch 10/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1265 - accuracy: 0.0000e+00 - val_loss: 1.2899 - val_accuracy: 0.0000e+00\n","Epoch 11/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1218 - accuracy: 0.0000e+00 - val_loss: 1.2860 - val_accuracy: 0.0000e+00\n","Epoch 12/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1172 - accuracy: 0.0000e+00 - val_loss: 1.2821 - val_accuracy: 0.0000e+00\n","Epoch 13/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1126 - accuracy: 0.0000e+00 - val_loss: 1.2784 - val_accuracy: 0.0000e+00\n","Epoch 14/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1082 - accuracy: 0.0000e+00 - val_loss: 1.2747 - val_accuracy: 0.0000e+00\n","Epoch 15/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.1038 - accuracy: 0.0000e+00 - val_loss: 1.2711 - val_accuracy: 0.0000e+00\n","Epoch 16/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0994 - accuracy: 0.0000e+00 - val_loss: 1.2675 - val_accuracy: 0.0000e+00\n","Epoch 17/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0952 - accuracy: 0.0000e+00 - val_loss: 1.2641 - val_accuracy: 0.0000e+00\n","Epoch 18/50\n","3/3 [==============================] - 0s 935us/step - loss: 1.0910 - accuracy: 0.0000e+00 - val_loss: 1.2607 - val_accuracy: 0.0000e+00\n","Epoch 19/50\n","3/3 [==============================] - 0s 948us/step - loss: 1.0868 - accuracy: 0.0000e+00 - val_loss: 1.2574 - val_accuracy: 0.0000e+00\n","Epoch 20/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0828 - accuracy: 0.0000e+00 - val_loss: 1.2541 - val_accuracy: 0.0000e+00\n","Epoch 21/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0787 - accuracy: 0.0000e+00 - val_loss: 1.2509 - val_accuracy: 0.0000e+00\n","Epoch 22/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0748 - accuracy: 0.0000e+00 - val_loss: 1.2478 - val_accuracy: 0.0000e+00\n","Epoch 23/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0709 - accuracy: 0.0000e+00 - val_loss: 1.2448 - val_accuracy: 0.0000e+00\n","Epoch 24/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0670 - accuracy: 0.0000e+00 - val_loss: 1.2418 - val_accuracy: 0.0000e+00\n","Epoch 25/50\n","3/3 [==============================] - 0s 901us/step - loss: 1.0632 - accuracy: 0.0000e+00 - val_loss: 1.2389 - val_accuracy: 0.0000e+00\n","Epoch 26/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0595 - accuracy: 0.0000e+00 - val_loss: 1.2360 - val_accuracy: 0.0000e+00\n","Epoch 27/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0558 - accuracy: 0.0000e+00 - val_loss: 1.2332 - val_accuracy: 0.0000e+00\n","Epoch 28/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0521 - accuracy: 0.0000e+00 - val_loss: 1.2304 - val_accuracy: 0.0000e+00\n","Epoch 29/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0485 - accuracy: 0.0000e+00 - val_loss: 1.2277 - val_accuracy: 0.0000e+00\n","Epoch 30/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0449 - accuracy: 0.0000e+00 - val_loss: 1.2251 - val_accuracy: 0.0000e+00\n","Epoch 31/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0414 - accuracy: 0.0000e+00 - val_loss: 1.2225 - val_accuracy: 0.0000e+00\n","Epoch 32/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0379 - accuracy: 0.0000e+00 - val_loss: 1.2200 - val_accuracy: 0.0000e+00\n","Epoch 33/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0345 - accuracy: 0.0000e+00 - val_loss: 1.2175 - val_accuracy: 0.0000e+00\n","Epoch 34/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0313 - accuracy: 0.0000e+00 - val_loss: 1.2163 - val_accuracy: 0.0000e+00\n","Epoch 35/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0297 - accuracy: 0.0000e+00 - val_loss: 1.2152 - val_accuracy: 0.0000e+00\n","Epoch 36/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0281 - accuracy: 0.0000e+00 - val_loss: 1.2140 - val_accuracy: 0.0000e+00\n","Epoch 37/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0265 - accuracy: 0.0000e+00 - val_loss: 1.2128 - val_accuracy: 0.0000e+00\n","Epoch 38/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0249 - accuracy: 0.0000e+00 - val_loss: 1.2117 - val_accuracy: 0.0000e+00\n","Epoch 39/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0233 - accuracy: 0.0000e+00 - val_loss: 1.2105 - val_accuracy: 0.0000e+00\n","Epoch 40/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0218 - accuracy: 0.0000e+00 - val_loss: 1.2094 - val_accuracy: 0.0000e+00\n","Epoch 41/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0202 - accuracy: 0.0000e+00 - val_loss: 1.2083 - val_accuracy: 0.0000e+00\n","Epoch 42/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0186 - accuracy: 0.0000e+00 - val_loss: 1.2072 - val_accuracy: 0.0000e+00\n","Epoch 43/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0171 - accuracy: 0.0000e+00 - val_loss: 1.2061 - val_accuracy: 0.0000e+00\n","Epoch 44/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0155 - accuracy: 0.0000e+00 - val_loss: 1.2051 - val_accuracy: 0.0000e+00\n","Epoch 45/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0140 - accuracy: 0.0000e+00 - val_loss: 1.2040 - val_accuracy: 0.0000e+00\n","Epoch 46/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0124 - accuracy: 0.0000e+00 - val_loss: 1.2030 - val_accuracy: 0.0000e+00\n","Epoch 47/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0109 - accuracy: 0.0000e+00 - val_loss: 1.2019 - val_accuracy: 0.0000e+00\n","Epoch 48/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0093 - accuracy: 0.0000e+00 - val_loss: 1.2009 - val_accuracy: 0.0000e+00\n","Epoch 49/50\n","3/3 [==============================] - 0s 2ms/step - loss: 1.0078 - accuracy: 0.0000e+00 - val_loss: 1.1999 - val_accuracy: 0.0000e+00\n","Epoch 50/50\n","3/3 [==============================] - 0s 1ms/step - loss: 1.0062 - accuracy: 0.0000e+00 - val_loss: 1.1989 - val_accuracy: 0.0000e+00\n"],"name":"stdout"}]}]}